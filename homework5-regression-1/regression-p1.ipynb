{"cells": [{"cell_type": "markdown", "id": "85774e22-a6fd-40db-894b-198cb4a93322", "metadata": {}, "source": ["# Homework 5: Regression, Part 1"]}, {"cell_type": "code", "execution_count": null, "id": "7997b8ba-d5d5-430f-b084-46f57466fd1c", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "import matplotlib.colors as mcolors\n", "from typing import Callable, List, Tuple"]}, {"cell_type": "markdown", "id": "ee16cfcb-3185-48ff-9dc8-aa6c0ef275be", "metadata": {}, "source": ["In this homework we'll be exploring the diabetes dataset to try to predict blood sugar level. To do this we'll be using linear regression models as our prediction methods. Next week the tutorial will build more advanced methods based off of this week's tutorial.\n", "\n", "This homework is dedicated to Wilford Brimley, who helped millions of Americans get their diabetes under control by promoting reasonably priced diabetes testing equipment supplied by Liberty Medical and Quaker Oats brand oatmeal, a great addition to a diabetes diet. In part thanks to Liberty Medical's testing equipment, the wonderful taste of Quaker Oats, and having sufficient income as a successful actor and brand spokesperson, thus affording him proper medial care, Brimley lived to the ripe old age of 85. \n", "\n", "<center>\n", "    <a href=\"https://www.youtube.com/watch?v=kyxBDARsGEw\">\n", "        <img src=\"wbrimley.jpg\" alt=\"GD image\" style=\"width:30%;\">\n", "    </a>    \n", "    \n", "    26 September 1934 -- 1 August 2020\n", "</center>"]}, {"cell_type": "markdown", "id": "7ed270c1-7f0a-4e5e-97de-cd18fd6bcb05", "metadata": {}, "source": ["## **Attention**: Expectations for this assignment\n", "\n", "We're a bit more than halfway through the course by now, which means we're removing some of the structure of previous assignments. Here's a list of the expectations we have:\n", "\n", "- You will have to write all of your own code. There is no skeleton code or tests.\n", "- You will need to write your own tests. We've provided some hints along the way as sanity checks, but these are no replacement for proper tests. Some of these functions might be difficult to come up with test cases for, but still, we would at least encourage you all to have a good hard think about how you could potentially test a given function or method.\n", "- You will need to write your own docstrings. We've included descriptions for each function we'd like you to write, but these are not docstrings. With every function, please also include a docstring, even if it's just copying the description that we've provided. Our solutions, when released will include numpy docstrings. \n", "- You will need to type all of your functions. This may be a requirement in the exam, and you might as well get good and make a habit of it while you have the opportunity to fail, than right before the exam.\n", "- You will need to add input testing to assure that your inputs are valid, and throw an exception otherwise. "]}, {"cell_type": "markdown", "id": "eea68d7f-2d9d-43a7-a554-b9e349c83350", "metadata": {}, "source": ["## Load the Data\n", "\n", "Write a function `load_diabetes` that loads the data from the file `diabetes.tab`. This is a tab-spaced value file, so when splitting each line, be sure to use `\\t`. \n", "\n", "This function should return a tuple in this order:\n", "\n", "- `data` -- A 2D numpy array that contains our data as floating point numbers, with individual data points being on the 0th axis and features being on the 1st axis.\n", "- `target` -- A 1D array that contains target values for each data point as floating point numbers. They're the last value in each row, and their feature label is `Y`.\n", "- `feature_labels` -- A list of strings that contain the labels for our features. They're the first line of the `.tab` file.\n", "\n", "Load the diabetes dataset by invoking this function. "]}, {"cell_type": "code", "execution_count": null, "id": "6b4c0388-931a-42cb-b93b-ad3dea46d1bd", "metadata": {}, "outputs": [], "source": ["\n", "### Please enter your solution here ###\n", "\n"]}, {"cell_type": "markdown", "id": "da545deb-197d-431c-b360-a64928cc0e21", "metadata": {}, "source": ["## Exploring the Data\n", "\n", "Write a function `print_table` that prints numerical values in a table with row and column labels. Each numerical value should have **three** decimal points. Each column should have a width equal to the number of the largest number of non-whitespace characters in a single row in the column. To do this, you will need to add a whitespace to most entries in a column. Columns are separated by `sep`.\n", "\n", "Parameters:\n", "\n", "- `row_names` -- A list of strings that label each row\n", "- `col_names` -- A list of string that label each column\n", "- `data` -- A list of numpy arrays, the same length of `col_names` where each numpy array has the same length as `row_names`\n", "- `sep` -- A string that separates each column. Default is three spaces.\n", "\n", "Return: None\n", "\n", "Write another function `print_stats_table` that uses `print_table` to print the mean, standard deviation, minimum value, maximum value, and range (max-min) of each feature. Use `print_stats_table` to print the statistics of the diabetes dataset. "]}, {"cell_type": "code", "execution_count": null, "id": "20bd3df8-6608-4873-aad6-9961f829fc73", "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "For example, it should print something like\n", "\n", "Feat    feat1   feat2\n", "row0    1.499   0.589\n", "row1    3.492   2.449\n", "\"\"\"\n", "\n", "\n", "### Please enter your solution here ###\n", "\n"]}, {"cell_type": "markdown", "id": "e7a05d39-ca4f-4a94-8f28-c7f1b4444e60", "metadata": {}, "source": ["### Pearson Correlation Coefficient \n", "\n", "This is wonderful information!... but it doesn't get us much closer to figuring out how were going to predict the target (which is blood sugar level). Let's see how much each feature correlates with the target feature. To do this, we'll calculate the Pearson correlation coefficient $r_{x,y}$ for each feature. A positive correlation indicates that the target tends to increase as the feature's value increases, and a negative correlation indicates that the target tends to decrease as the feature's value increases. The magnitude of the correlation coefficient indicates to what extent this occurs. If you care to know more, [here's](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) a link to the Wikipedia page for Pearson correlation. The formula for this metric given two vectors $x$ and $y$ is:\n", "\n", "$$ r_{x,y} = \\frac{\\sum^n_{i=1} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum^n_{i=1}(x_i-\\bar{x})^2}\\sqrt{\\sum^n_{i=1}(y_i-\\bar{y})^2}} $$\n", "where the sample mean is defined as\n", "$$ \\bar{x} = \\frac{1}{n} \\sum_{i = 1}^n x_i $$\n", "with $\\bar{y}$ defined similarly with respect to $y$.\n", "\n", "Write the function `pearson_correlation` that computes the Pearson correlation coefficients with the following arguments and return values. Do **not** use `np.corrcoef` in your implementation, but feel free to use it to check your work.\n", "\n", "Arguments:\n", "- `X` A matrix of datapoints with individual datapoints in the rows and features in the columns\n", "- `Y` A matrix or vector of datapoints to find the correlation with `X`\n", "  \n", "Return:\n", "- Numpy array with the Pearson correlation coefficients\n", "\n", "Print the Pearson correlation coefficients between each feature and the target values in a table with feature labels as the rows and the Pearson correlations in the first column."]}, {"cell_type": "code", "execution_count": null, "id": "ea786aa5-51a6-4686-bc4b-a26f58ccdc1d", "metadata": {}, "outputs": [], "source": ["\n", "### Please enter your solution here ###\n", "\n"]}, {"cell_type": "markdown", "id": "d3d5b449-57f4-40a0-9348-0a0cf5125745", "metadata": {}, "source": ["If you did this correctly, the highest correlation should be `bmi` at 0.586, and the lowest `s3` at -0.395. "]}, {"cell_type": "markdown", "id": "904add27-ea17-4fda-8f31-592aab83d75d", "metadata": {}, "source": ["### Pearson Correlation Matrix\n", "\n"]}, {"cell_type": "markdown", "id": "91647097-7a51-4985-8729-8a1218368a6c", "metadata": {}, "source": ["Write a function `plot_matrix` that plots the covariance matrix as a heatmap along a scale that goes from -1 to 1 and includes feature labels on the left and top of the plot. The labels on top should be slanted at a 45 degree angle. Plot the covariance matrix."]}, {"cell_type": "code", "execution_count": null, "id": "94fedafa-03d1-4e60-b0c8-e02025b40832", "metadata": {}, "outputs": [], "source": ["\n", "### Please enter your solution here ###\n", "\n"]}, {"cell_type": "markdown", "id": "955e7024-1983-4131-94af-59a0fa31478b", "metadata": {}, "source": ["If you did this correctly, aside from those on the diagonal, you should see `s1` and `s2` have the highest correlation a bit above 0.75, and the lowest should be between `s3` and `s4` between -0.5 and -0.75."]}, {"cell_type": "markdown", "id": "dfa8d628-0e9a-400a-9d35-2c74ce773b94", "metadata": {}, "source": ["## Linear Regression\n", "\n", "For this task, we will be exploring fitting data to a linear model $\\hat{y}_i = \\theta^\\top x_i + \\theta_0$. The goal of fitting is to find the parameters $\\theta$ and $\\theta_0$. \n", "\n", "By using a linear model we are making the following assumptions about our data. \n", "\n", "- **Linearity** -- The target variable can be represented as a linear combination of input features, or otherwise said that the target variable was generated by a linear process of all input features, plus some noise.\n", "- **Independence** -- Each datapoint, or more importantly the intrinsic noise of each datapoint, are independent of one another.\n", "- **Homoscedasticity** -- The intrinsic noise, or rather the error, of all datapoints has a constant variance.\n", "- **No multicollinearity** -- Each feature is not perfectly correlated with one another.\n", "- **Zero mean of residuals** -- The mean of the error, or intrinsic noise, is 0.\n", "\n", "To fit a model to data we need a metric that tells us how wrong the current model is:\n", "\n", "A **loss function** determines how we determine the error for a single prediction, and for this we'll be using squared error. $$L(x_i, y_i, \\theta, \\theta_0) = (y_i - \\hat{y}_i)^2 = (y_i - (\\theta^\\top x_i + \\theta_0))^2$$\n", "\n", "An **objective function** is the function we'll be minimizing, which in this case will be mean squared error (MSE). $$J(X, Y, \\theta, \\theta_0) = \\frac{1}{N}\\sum^N_{i=1}L(x_i, y_i, \\theta, \\theta_0) = \\frac{1}{N}\\sum^N_{i=1}(y_i - \\hat{y}_i)^2$$\n", "\n", "In practice you'll often see loss function, objective function, error function, cost function, among other similar terms used interchangeably. \n", "\n", "Generally, you might see this optimization task denoted as:\n", "\n", "$$\\theta, \\theta_0 = \\underset{\\theta, \\theta_0}{\\min} J(X, Y, \\theta, \\theta_0)$$\n", "\n", "Luckily, for linear regression with mean squared error there's a closed-form solution, creating an ordinary least-squares (OLS) estimator.\n", "\n", "$$\\theta, \\theta_0 = (X^\\top X)^{-1}X^\\top y$$\n", "\n", "\n", "To account for the bias term $\\theta_0$, we will need to add a feature to our data set where every value is $1$. "]}, {"cell_type": "markdown", "id": "c9236d6c-7906-4665-91c4-b03995679185", "metadata": {}, "source": ["Write a function called `least_squares` that has the following parameters and return values:\n", "\n", "Parameters:\n", "- `X` -- Data points as a numpy array\n", "- `Y` -- Target values as a numpy array\n", "\n", "Returns: Parameter vector as a numpy array\n", "\n", "Feel free to use `np.linalg.inv()`.\n", "\n", "Write another function called `mse` that calculates the mean squared error given our target variable $y$ and our predictions $\\hat{y}$ as numpy arrays."]}, {"cell_type": "code", "execution_count": null, "id": "6192e282-01ea-4541-9458-62951071c67d", "metadata": {}, "outputs": [], "source": ["\n", "### Please enter your solution here ###\n", "\n"]}, {"cell_type": "markdown", "id": "25215c75-3c06-4ddc-9ea7-4b853de99bc7", "metadata": {}, "source": ["Using `least_squares`, calculate the parameters of the model. Don't forget to add a column of 1's for the bias term. Print the model weights and the Pearson correlations between each feature and the target in the same table. Set the Pearson correlation for the bias term to a silly value, such as 42, since there is no Pearson correlation for the bias term. Also, print the MSE for the fit model."]}, {"cell_type": "code", "execution_count": null, "id": "9ede8999-0d41-4710-bdec-2f38584a5e9e", "metadata": {}, "outputs": [], "source": ["\n", "### Please enter your solution here ###\n", "\n"]}, {"cell_type": "markdown", "id": "cedcb70e-ff00-4fc9-826c-b1db407e412c", "metadata": {}, "source": ["Often when working with a datasets we'll standardize or normalize the data. Some methods specifically require this. In the case of the diabetes dataset, it shouldn't do anything. \n", "\n", "- **Normalization** -- This linearly shifts the range of each feature to a predetermined range, usually $[0,1]$ or $[-1, 1]$.\n", "\n", "The formula for the interval $[0, 1]$ looks like:\n", "$$ x' = \\frac{x - x_{min}}{x_{max} - x_{min}} $$\n", "\n", "and the formula for the interval $[-1, 1]$ looks like:\n", "$$ x' = 2 * \\frac{x - x_{min}}{x_{max} - x_{min}} - 1$$\n", "  \n", "- **Standardization** -- This centers the data so the mean of each feature is $0$ and the standard deviation of each feature is $1$. The formula for this looks like:\n", "\n", "    $$ x' = \\frac{x - \\bar{x}}{\\sigma(x)} $$\n", "\n", "Write three functions `normalize_0_1`, `normalize_1_1` and `standardize`, both which take the data array, and return that array in a normalized form in the ranges $[0,1]$ or $[-1,1]$, or a standardized form, respectively. For each modified dataset print a table with statistics on each feature, as you did after you loaded the dataset, and also use ordinary least squares to find the model parameters for each modified dataset and calculate the MSE. Print these parameters and the MSE. Don't forget to add the columns of $1$s for the bias term. "]}, {"cell_type": "code", "execution_count": null, "id": "1dd2b038-929e-4356-83a6-ff6fd12c9dd7", "metadata": {}, "outputs": [], "source": ["\n", "### Please enter your solution here ###\n", "\n"]}, {"cell_type": "markdown", "id": "958abe43-29ca-48ed-838b-4bc613f9ca7a", "metadata": {}, "source": ["## Ridge Regression\n", "\n", "Often, in machine learning tasks, we'll find that the weights of our model become extremely large. This is often a result of **overfitting**, or rather where the model memorizes the data that we've trained on. This would be fine if our model only ever encountered our training data, but this is almost never the case in practice. When a model overfits on its training data, it often fails to **generalize** to new data. To help a model generalize we're going to introduce a **regularization** term. There are two main types of regularization:\n", "\n", "**L1 Regularization** -- Also known as LASSO when used with linear regression, where we add the sum absolute values of all coefficients to the objective function, multiplied by the hyperparameter $\\lambda$. This encourages sparser models, pushing most parameters towards 0.\n", "\n", "$$ J(X, Y, \\theta, \\theta_0) = \\frac{1}{n}\\sum^n_{i=1}L(x_i, y_i, \\theta, \\theta_0) + \\lambda |[\\theta,\\theta_0]| = \\frac{1}{n}\\sum^n_{i=1}(y_i - \\hat{y}_i)^2  + \\lambda |[\\theta,\\theta_0]|$$ \n", "\n", "**L2 Regularization** -- Also called ridge regression when used with linear regression, where we add the sum of the squares of all parameters, multiplied by hyperparameter $\\lambda$, to the objective function. \n", "\n", "$$ J(X, Y, \\theta, \\theta_0) = \\frac{1}{n}\\sum^n_{i=1}L(x_i, y_i, \\theta, \\theta_0) + \\lambda \\|[\\theta,\\theta_0]\\|^2 = \\frac{1}{n}\\sum^n_{i=1}(y_i - \\hat{y}_i)^2  + \\lambda \\|[\\theta,\\theta_0]\\|^2$$ \n", "\n", "There is no closed-form solution for LASSO. Use L1 regularization with a linear regression model and you have to use an optimization method. Ridge regression, however, has a nice closed-form solution:\n", "\n", "$$\\theta, \\theta_0 = (X^\\top X + \\lambda I)^{-1}X^\\top Y$$\n", "\n", "Write a function `ridge_regression` that calculates $\\theta$ and $\\theta_0$. This should have an identical arguments and return types as `least_squares` with the exception of an additional parameter `lam`, which is a float."]}, {"cell_type": "code", "execution_count": null, "id": "ec4bdf28-bd7b-4c99-be74-22b80d5d8ce2", "metadata": {}, "outputs": [], "source": ["\n", "### Please enter your solution here ###\n", "\n"]}, {"cell_type": "markdown", "id": "5dc88717-4bfe-4730-9575-19e3011db8ea", "metadata": {}, "source": ["From this point on we will be using standardized data. Feel free to overwrite your data term. Find the parameters of the model using ridge regression and a $\\lambda$ value of 0.2. "]}, {"cell_type": "code", "execution_count": null, "id": "6960fb7f-42dd-40d3-a609-a8dbd96478d0", "metadata": {}, "outputs": [], "source": ["\n", "### Please enter your solution here ###\n", "\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.12"}}, "nbformat": 4, "nbformat_minor": 5}